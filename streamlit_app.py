import streamlit as st
from openai import OpenAI
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import Qdrant
from langchain.chains import RetrievalQA
from langchain import PromptTemplate
from pathlib import Path
import json
from langchain.text_splitter import CharacterTextSplitter
import tiktoken
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

# Qdrantã®è¨­å®š
COLLECTION_NAME = "gci_2024_winter"

# ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜ã‚’è¡¨ç¤º
st.title("ğŸ’¬ RAGæ©Ÿèƒ½ä»˜ããƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
st.write(
    "ã“ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯ã€OpenAIã®GPT-3.5ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ã¦ã„ã¾ã™ã€‚ "
    "ã“ã®ã‚¢ãƒ—ãƒªã‚’ä½¿ã†ã«ã¯ã€OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚APIã‚­ãƒ¼ã¯[ã“ã¡ã‚‰](https://platform.openai.com/account/api-keys)ã§å–å¾—ã§ãã¾ã™ã€‚"
)

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰OpenAI APIã‚­ãƒ¼ã‚’å–å¾—
openai_api_key = st.text_input("OpenAI APIã‚­ãƒ¼", type="password")

@st.cache_resource
def get_qdrant_client():
    return QdrantClient(":memory:")  # ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ¼ãƒ‰ã§åˆæœŸåŒ–

def load_qdrant(client):
    collections = client.get_collections().collections
    collection_names = [collection.name for collection in collections]
    if COLLECTION_NAME not in collection_names:
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
        )
        st.write('æ–°ã—ã„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¾ã—ãŸã€‚')
    return Qdrant(
        client=client,
        collection_name=COLLECTION_NAME, 
        embeddings=OpenAIEmbeddings(model="text-embedding-3-small", api_key=openai_api_key)
    )

def build_vector_store(texts, client):
    qdrant = load_qdrant(client)
    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
        model_name="text-embedding-3-small",
        chunk_size=250,
        chunk_overlap=0,
    )
    documents = text_splitter.create_documents(texts)
    qdrant.add_documents(documents)
    st.write('ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚')
    return qdrant

if not openai_api_key:
    st.info("APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚", icon="ğŸ—ï¸")
else:
    # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è¨­å®š
    client = OpenAI(api_key=openai_api_key)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    workdir = Path('.')  # å®Ÿéš›ã®ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç½®ãæ›ãˆã¦ãã ã•ã„
    datapath = workdir / "gci.json"
    
    with datapath.open('r', encoding='utf-8') as file:
        data = json.load(file)

    # JSONãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
    texts = [json.dumps(d, ensure_ascii=False) for d in data]
    st.write(f"èª­ã¿è¾¼ã‚“ã ãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(texts)}")

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚µã‚¤ã‚ºï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰ã‚’ç¢ºèª
    encoding = tiktoken.encoding_for_model('text-embedding-3-small')
    len_docs = [len(encoding.encode(text)) for text in texts]

    avg_doc_length = round(sum(len_docs) / len(len_docs), 1)
    max_doc_length = max(len_docs)

    st.write(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {avg_doc_length}")
    st.write(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_doc_length}")

    if max_doc_length > 5000:
        st.warning("> ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒ5,000ã‚’è¶…ãˆã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå­˜åœ¨ã™ã‚‹ã®ã§ã€ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æŠ‘ãˆã‚‹ä½•ã‚‰ã‹ã®å¯¾å‡¦ãŒå¿…è¦", icon="âš ï¸")

    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰
    build_vector_store(texts)

    # ãƒ¬ãƒˆãƒªãƒ¼ãƒãƒ¼ã®è¨­å®š
    qdrant = load_qdrant()
    retriever = qdrant.as_retriever(search_kwargs={"k": 3})

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å®šç¾©
    template = """
    ã‚ãªãŸã¯GCI 2024 Winterã®è¬›å¸«ã§ã™ã€‚
    
    æ¬¡ã®å‚è€ƒæƒ…å ±ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
    
    # åˆ¶ç´„
    äº‹å®Ÿã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’åˆ†ã‘ã¦æ•™ãˆã¦ãã ã•ã„ã€‚
    
    # å‚è€ƒæƒ…å ±
    {context}
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
    {question}
    
    # å‡ºåŠ›
    äº‹å®Ÿï¼š
    ã‚¢ãƒ‰ãƒã‚¤ã‚¹ï¼š
    å›ç­”æ–‡æ›¸ï¼š
    """
    
    PROMPT = PromptTemplate(
        template=template,
        input_variables=["context", "question"],
        template_format="f-string"
    )
    
    # LLMã®è¨­å®š
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", api_key=openai_api_key, temperature=0.0)
    
    # RetrievalQAãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT},
        verbose=True,
    )
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿å­˜
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # éå»ã®ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•å…¥åŠ›
    if prompt := st.chat_input("è³ªå•ã‚’ã©ã†ã"):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’ä¿å­˜ã—ã¦è¡¨ç¤º
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # RAGãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦å›ç­”ã‚’ç”Ÿæˆ
        response = qa.invoke(prompt)
        
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å›ç­”ã‚’è¡¨ç¤ºã—ã¦ä¿å­˜
        answer = response['result']
        st.session_state.messages.append({"role": "assistant", "content": answer})
        with st.chat_message("assistant"):
            st.markdown(answer)